import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
import os

# ================= è¨­å®šå€ =================
MODEL_ID = "Qwen/Qwen2-VL-2B-Instruct" 

_global_model = None
_global_processor = None

def load_model():
    global _global_model, _global_processor
    if _global_model is not None:
        return

    print(f"ğŸš€ [OCR Service] æ­£åœ¨å•Ÿå‹•è¼•é‡ç´šæ¨¡å‹ ({MODEL_ID})...")
    
    try:
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            MODEL_ID,
            torch_dtype="auto",
            device_map="auto", 
            trust_remote_code=True,
        )
        processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
        
        _global_model = model
        _global_processor = processor
        print("âœ… [OCR Service] æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ [OCR Service] æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        raise e

def extract_text_from_image(image_path):
    global _global_model, _global_processor

    print(f"[OCR Service] æ”¶åˆ°è«‹æ±‚ï¼Œè™•ç†åœ–ç‰‡ï¼š{image_path}")

    if _global_model is None:
        try:
            load_model()
        except:
            return "éŒ¯èª¤ï¼šæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥å¾Œç«¯ logsã€‚"

    if not os.path.exists(image_path):
        return "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åœ–ç‰‡æª”æ¡ˆ"

    try:
        image = Image.open(image_path).convert("RGB") 
        
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Identify and transcribe the text in this resume into structured Markdown format. Keep the original language (Traditional Chinese). DO NOT TRANSLATE."}
            ]
        }]
        
        # 1. æº–å‚™è¼¸å…¥
        text_input = _global_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = _global_processor(text=[text_input], images=[image], return_tensors="pt")
        inputs = {k: v.to(_global_model.device) for k, v in inputs.items()}
        
        # 2. æ¨è«–
        generated_ids = _global_model.generate(**inputs, max_new_tokens=1500)
        
        # --- ã€é—œéµä¿®æ­£ã€‘ å‰ªæ‰å‰é¢çš„ Prompt ---
        # è¨ˆç®—è¼¸å…¥ tokens çš„é•·åº¦
        input_token_len = inputs['input_ids'].shape[1]
        
        # åªä¿ç•™è¼¸å…¥é•·åº¦ä¹‹å¾Œçš„ tokens (ä¹Ÿå°±æ˜¯ AI æ–°å¯«çš„éƒ¨åˆ†)
        generated_ids_trimmed = generated_ids[:, input_token_len:]
        
        # 3. è§£ç¢¼ (ç¾åœ¨åªæœƒè§£å‡ºç´”æ·¨çš„å±¥æ­·å…§å®¹)
        generated_text = _global_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]
        # ------------------------------------
        
        image.close()
        print("[OCR Service] è¾¨è­˜å®Œæˆï¼")
        return generated_text

    except Exception as e:
        print(f"[OCR Service] ç™¼ç”ŸéŒ¯èª¤: {e}")
        return f"OCR è¾¨è­˜å¤±æ•—: {str(e)}"