import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
import os
import fitz  # PyMuPDF: ç”¨ä¾†è™•ç† PDF

# ================= è¨­å®šå€ =================
MODEL_ID = "Qwen/Qwen2-VL-2B-Instruct" 

_global_model = None
_global_processor = None

def load_model():
    global _global_model, _global_processor
    if _global_model is not None:
        return

    print(f"ğŸš€ [OCR Service] æ­£åœ¨å•Ÿå‹•è¼•é‡ç´šæ¨¡å‹ ({MODEL_ID})...")
    
    try:
        # åŠ å…¥é¡¯å­˜å„ªåŒ–è¨­å®š
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            MODEL_ID,
            torch_dtype="auto",
            device_map="auto", 
            trust_remote_code=True,
        )
        processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
        
        _global_model = model
        _global_processor = processor
        print("âœ… [OCR Service] æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ [OCR Service] æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        raise e

# === æ–°å¢åŠŸèƒ½: PDF è½‰åœ–ç‰‡ ===
def _convert_pdf_to_image(pdf_path):
    # ... (å‰é¢çš„ code ä¸è®Š) ...
    
    doc = fitz.open(pdf_path)
    images = []

    # ã€ä¿®æ”¹é€™è£¡ã€‘è¨­å®šæœ€å¤§é æ•¸é™åˆ¶ï¼Œä¾‹å¦‚åªè®€å‰ 3 é 
    max_pages = 3 
    total_pages = len(doc)
    pages_to_process = min(total_pages, max_pages)

    if total_pages > max_pages:
        print(f"âš ï¸ [OCR Service] PDF é æ•¸éå¤š ({total_pages} é )ï¼Œåƒ…è®€å–å‰ {max_pages} é ä»¥é¿å…é¡¯å­˜çˆ†ç‚¸ã€‚")

    for page_num in range(pages_to_process): # æ”¹ç”¨ pages_to_process
        page = doc.load_page(page_num)
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2)) 
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    
    doc.close()

    # ... (å¾Œé¢çš„æ‹¼æ¥ code ä¸è®Š) ...

    if not images:
        raise ValueError("PDF æª”æ¡ˆæ˜¯ç©ºçš„æˆ–ç„¡æ³•è®€å–")

    # å¦‚æœåªæœ‰ä¸€é ï¼Œç›´æ¥å›å‚³
    if len(images) == 1:
        return images[0]

    # å¦‚æœæœ‰å¤šé ï¼Œå°‡å®ƒå€‘å‚ç›´æ‹¼æ¥
    total_width = max(img.width for img in images)
    total_height = sum(img.height for img in images)
    
    # å»ºç«‹ä¸€å¼µæ–°çš„é•·ç•«å¸ƒ
    combined_image = Image.new('RGB', (total_width, total_height), (255, 255, 255))
    
    y_offset = 0
    for img in images:
        # ç‚ºäº†ç¾è§€ï¼Œå°‡åœ–ç‰‡ç½®ä¸­è²¼ä¸Š (é›–ç„¶é€šå¸¸å¯¬åº¦éƒ½ä¸€æ¨£)
        x_offset = (total_width - img.width) // 2
        combined_image.paste(img, (x_offset, y_offset))
        y_offset += img.height
    
    print(f"[OCR Service] PDF è½‰æ›å®Œæˆï¼Œæ‹¼æ¥äº† {len(images)} é ")
    return combined_image

def extract_text_from_image(file_path):
    global _global_model, _global_processor

    print(f"[OCR Service] æ”¶åˆ°è«‹æ±‚ï¼Œè™•ç†æª”æ¡ˆï¼š{file_path}")

    if _global_model is None:
        try:
            load_model()
        except:
            return "éŒ¯èª¤ï¼šæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥å¾Œç«¯ logsã€‚"

    if not os.path.exists(file_path):
        return "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ"

    image = None
    try:
        # === ä¿®æ”¹æ ¸å¿ƒï¼šåˆ¤æ–·å‰¯æª”å ===
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            image = _convert_pdf_to_image(file_path)
        else:
            # åŸæœ¬çš„åœ–ç‰‡è™•ç†é‚è¼¯
            image = Image.open(file_path).convert("RGB") 
        
        # ===========================
        
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Identify and transcribe the text in this resume into structured Markdown format. Keep the original language (Traditional Chinese). DO NOT TRANSLATE."}
            ]
        }]
        
        # 1. æº–å‚™è¼¸å…¥
        text_input = _global_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = _global_processor(text=[text_input], images=[image], return_tensors="pt")
        inputs = {k: v.to(_global_model.device) for k, v in inputs.items()}
        
        # 2. æ¨è«–
        generated_ids = _global_model.generate(**inputs, max_new_tokens=1500)
        
        # å‰ªæ‰ Prompt
        input_token_len = inputs['input_ids'].shape[1]
        generated_ids_trimmed = generated_ids[:, input_token_len:]
        
        # 3. è§£ç¢¼
        generated_text = _global_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]
        
        # é¡¯å¼é—œé–‰åœ–ç‰‡é‡‹æ”¾è³‡æº (é›–ç„¶ Python æœƒè‡ªå‹•å›æ”¶ï¼Œä½†å¥½ç¿’æ…£)
        if hasattr(image, 'close'):
            image.close()

        print("[OCR Service] è¾¨è­˜å®Œæˆï¼")
        return generated_text

    except Exception as e:
        print(f"[OCR Service] ç™¼ç”ŸéŒ¯èª¤: {e}")
        # ç™¼ç”ŸéŒ¯èª¤æ™‚å˜—è©¦æ¸…ç†é¡¯å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return f"OCR è¾¨è­˜å¤±æ•—: {str(e)}"